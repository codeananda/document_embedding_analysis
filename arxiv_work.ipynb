{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'long_list': [0, 1, 2, 3, 4, '...'],\n",
      " 'long_string': 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
      " 'nested': {'nested_list': [0, 1, 2, 3, 4, '...'],\n",
      "            'nested_string': 'bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)rocessor_config.json:   0%|          | 0.00/275 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "304270defbc64536a8a1141ff325f017"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/348 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcf146396a474be386a4e4d79395dae2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2cd1bb0696d49e5a08538caec72e2a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a0c55abe89f4f29bfea8ec0cebd8238"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d619b6c552945f3aed6bcbba3b77b6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f98ee335e73e4dfd8575908e40970452"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa181767c1904ae9a90a71047ceb9c07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'LayoutLMv3TokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1da89e39e20749fd8142aa6f3c50ae71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path\n",
    "from pprint import pprint, PrettyPrinter\n",
    "from time import time, sleep\n",
    "from typing import List, Dict\n",
    "from uuid import uuid4\n",
    "from collections import defaultdict\n",
    "\n",
    "import evaluate\n",
    "import openai\n",
    "import requests\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from doctran import Doctran, ExtractProperty\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from evaluate import load\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import (\n",
    "    OpenAIEmbeddings,\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import (\n",
    "    MarkdownTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    LineType,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForTokenClassification\n",
    "from loguru import logger\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from main import (\n",
    "    extract_plan_and_content_wikipedia,\n",
    "    compare_documents_plans,\n",
    "    compare_documents_sections,\n",
    "    extract_plan_and_content_patent,\n",
    ")\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm_default = ChatOpenAI(model_name=\"gpt-3.5-turbo\", streaming=True)\n",
    "llm_16k = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", streaming=True)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "    except ValueError:\n",
    "        encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def convert_to_markdown(article_dict):\n",
    "    md_text = \"\"\n",
    "\n",
    "    for heading, content in article_dict.items():\n",
    "        # heading is of form: 'h3 Example'\n",
    "        # Define the markdown equivalent for the heading level\n",
    "        heading_level = \"#\" * int(heading[1])\n",
    "        heading = heading[3:]\n",
    "        # Append the heading and the content to the markdown text\n",
    "        md_text += f\"{heading_level} {heading}\\n\\n{content}\\n\\n\"\n",
    "\n",
    "    return md_text\n",
    "\n",
    "\n",
    "def truncated_pprint(obj, N=5):\n",
    "    \"\"\"Pretty print an object, truncating lists and strings to N items/characters\n",
    "    for easier viewing of plan_json objects\"\"\"\n",
    "    def truncate(item, N):\n",
    "        if isinstance(item, list) and N is not None:\n",
    "            return item[:N] + (['...'] if len(item) > N else [])\n",
    "        if isinstance(item, str) and N is not None:\n",
    "            N = 125\n",
    "            return item[:N] + ('...' if len(item) > N else '')\n",
    "        return item\n",
    "\n",
    "    def trunc_recursive(item, N):\n",
    "        if isinstance(item, list):\n",
    "            return [trunc_recursive(i, N) for i in truncate(item, N)]\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: trunc_recursive(v, N) for k, v in item.items()}\n",
    "        else:\n",
    "            return truncate(item, N)\n",
    "\n",
    "    truncated_obj = trunc_recursive(obj, N)\n",
    "    pprint(truncated_obj, sort_dicts=False)\n",
    "\n",
    "# Test\n",
    "data = {\n",
    "    'long_list': list(range(100)),\n",
    "    'long_string': 'a' * 100,\n",
    "    'nested': {\n",
    "        'nested_list': list(range(50)),\n",
    "        'nested_string': 'b' * 50\n",
    "    }\n",
    "}\n",
    "\n",
    "truncated_pprint(data, 5)\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"nielsr/layoutlmv3-finetuned-funsd\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"nielsr/layoutlmv3-finetuned-funsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "arxiv_papers = [\n",
    "    \"https://arxiv.org/pdf/2307.04438.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.14697.pdf\",\n",
    "    \"https://arxiv.org/pdf/2302.09051.pdf\",\n",
    "    \"https://arxiv.org/pdf/2305.10091.pdf\",\n",
    "    \"https://arxiv.org/pdf/2305.17474.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.16960.pdf\",\n",
    "    \"https://arxiv.org/pdf/2305.20069.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.08451.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.17003.pdf\",\n",
    "    \"https://arxiv.org/pdf/2307.07573.pdf\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Convert string to a valid filename.\"\"\"\n",
    "    s = str(filename).strip().replace(' ', '_')\n",
    "    # Remove any character that is not a word character\n",
    "    # (alphanumeric + underscore), not a hyphen, or not a period.\n",
    "    return re.sub(r'(?u)[^-\\w.]', '', s)\n",
    "\n",
    "def get_arxiv_metadata(arxiv_id):\n",
    "    \"\"\"Fetch metadata for the given arXiv ID.\"\"\"\n",
    "    url = f'https://export.arxiv.org/api/query?id_list={arxiv_id}'\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.text\n",
    "\n",
    "    # Use regex to extract the title. There are better ways (like parsing XML),\n",
    "    # but this is simple and should work for our purpose.\n",
    "    match = re.search(r'<title>([^<]+)</title>', data)\n",
    "    title = match.group(1) if match else None\n",
    "    return {'title': title}\n",
    "\n",
    "def download_arxiv_pdf(arxiv_url):\n",
    "    \"\"\"Given a arxiv_url, download the PDF to the data/arxiv directory.\"\"\"\n",
    "    arxiv_id = arxiv_url.split('/')[-1].replace('.pdf', '')\n",
    "    metadata = get_arxiv_metadata(arxiv_id)\n",
    "\n",
    "    if metadata.get('title'):\n",
    "        filename = sanitize_filename(metadata['title']) + '.pdf'\n",
    "    else:\n",
    "        filename = f\"{arxiv_id}.pdf\"\n",
    "\n",
    "    arxiv_dir = Path('data/arxiv')\n",
    "    os.makedirs(arxiv_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(arxiv_url)\n",
    "    output_file = arxiv_dir / filename\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded to {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "arxiv_url = 'https://arxiv.org/pdf/2306.14697.pdf'\n",
    "# download_arxiv_pdf(arxiv_url)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to data\\arxiv\\Reconfigurable_Intelligent_Surface_Assisted_Railway_Communications_A__survey.pdf\n",
      "Downloaded to data\\arxiv\\A_Survey_of_Software-Defined_Smart_Grid_Networks_Security_Threats_and__Defense_Techniques.pdf\n",
      "Downloaded to data\\arxiv\\Complex_QA_and_language_models_hybrid_architectures_Survey.pdf\n",
      "Downloaded to data\\arxiv\\Multi-Agent_Reinforcement_Learning_Methods_Applications_Visionary__Prospects_and_Challenges.pdf\n",
      "Downloaded to data\\arxiv\\Macroeconomic_Effects_of_Inflation_Targeting_A_Survey_of_the_Empirical__Literature.pdf\n",
      "Downloaded to data\\arxiv\\Sketching_a_Model_on_Fisheries_Enforcement_and_Compliance_--_A_Survey.pdf\n",
      "Downloaded to data\\arxiv\\A_survey_on_the_complexity_of_learning_quantum_states.pdf\n",
      "Downloaded to data\\arxiv\\A_Survey_on_Blood_Pressure_Measurement_Technologies_Addressing__Potential_Sources_of_Bias.pdf\n",
      "Downloaded to data\\arxiv\\A_survey_on_algebraic_dilatations.pdf\n",
      "Downloaded to data\\arxiv\\Literature_Survey_on_the_Container_Stowage_Planning_Problem.pdf\n"
     ]
    }
   ],
   "source": [
    "for paper in arxiv_papers:\n",
    "    download_arxiv_pdf(paper)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
